---
---

@inproceedings{liu2024attribotbagtricksefficiently,
      abbr={ICLR},
      title={AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution}, 
      author={Fengyuan Liu and Nikhil Kandpal and Colin Raffel},
      year={2025},
      booktitle={The Thirteenth International Conference on Learning Representations},
      url={https://arxiv.org/abs/2411.15102}, 
      selected={true},
      pdf={AttriBoT A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution.pdf},
      arxiv={2411.15102},
      code={https://github.com/r-three/AttriBoT},
      abstract={The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a >300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response 30x faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.}
}

@article{lin2025efficientmodeldevelopmentfinetuning,
      abbr={arXiv},
      title={Efficient Model Development through Fine-tuning Transfer}, 
      author={Pin-Jie Lin and Rishab Balasubramanian and Fengyuan Liu and Nikhil Kandpal and Tu Vu},
      year={2025},
      url={https://arxiv.org/abs/2503.20110}, 
      selected={true},
      pdf={Efficient Model Development through Fine-tuning Transfer.pdf},
      arxiv={2503.20110},
      code={https://github.com/pjlintw/finetuning-transfer},
      abstract={Modern LLMs struggle with efficient updates, as each new pretrained model version requires repeating expensive alignment processes. This challenge also applies to domain- or language-specific models, where fine-tuning on specialized data must be redone for every new base model release. In this paper, we explore the transfer of fine-tuning updates between model versions. Specifically, we derive the diff vector from one source model version, which represents the weight changes from fine-tuning, and apply it to the base model of a different target version. Through empirical evaluations on various open-weight model versions, we show that transferring diff vectors can significantly improve the target base model, often achieving performance comparable to its fine-tuned counterpart. For example, reusing the fine-tuning updates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on GPQA over the base Llama 3.1 8B without additional training, surpassing Llama 3.1 8B Instruct. In a multilingual model development setting, we show that this approach can significantly increase performance on target-language tasks without retraining, achieving an absolute improvement of 4.7% and 15.5% on Global MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B Instruct. Our controlled experiments reveal that fine-tuning transfer is most effective when the source and target models are linearly connected in the parameter space. Additionally, we demonstrate that fine-tuning transfer offers a stronger and more computationally efficient starting point for further fine-tuning. Finally, we propose an iterative recycling-then-finetuning approach for continuous model development, which improves both efficiency and effectiveness. Our findings suggest that fine-tuning transfer is a viable strategy to reduce training costs while maintaining model performance.}
}